"""
Script to prepare the complete dataset with train/val/test splits and text explanations.
"""

import sys
from pathlib import Path
import argparse
import logging
import pandas as pd

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.data.preprocessing import DataPreprocessor
from src.templates.template_engine import TemplateEngine
from src.templates.vocabulary import build_vocabulary_from_templates


def setup_logging():
    """Setup logging configuration."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )


def generate_explanations(df: pd.DataFrame, template_engine: TemplateEngine) -> pd.DataFrame:
    """
    Generate text explanations for each sample.

    Args:
        df: DataFrame with disease and sensor data
        template_engine: Template engine for generating explanations

    Returns:
        DataFrame with added 'explanation' column
    """
    logger = logging.getLogger(__name__)

    logger.info("Generating text explanations...")

    explanations = template_engine.generate_batch_explanations(
        diseases=df['disease'].tolist(),
        temperatures=df['temperature'].values,
        humidities=df['humidity'].values,
        soil_moistures=df['soil_moisture'].values,
        include_recommendation=False
    )

    df = df.copy()
    df['explanation'] = explanations

    # Log some samples
    logger.info("\nSample explanations:")
    for i, row in df.head(3).iterrows():
        logger.info(f"\n{row['disease']}:")
        logger.info(f"  {row['explanation'][:150]}...")

    return df


def main():
    """Main function."""
    parser = argparse.ArgumentParser(
        description='Prepare dataset with train/val/test splits'
    )

    parser.add_argument(
        '--sensor-data',
        type=str,
        default='data/synthetic/sensor_data.csv',
        help='Path to sensor data CSV (generated by generate_synthetic_data.py)'
    )

    parser.add_argument(
        '--output-dir',
        type=str,
        default='data/processed/splits',
        help='Output directory for train/val/test splits'
    )

    parser.add_argument(
        '--vocab-output',
        type=str,
        default='data/processed/vocabulary.pkl',
        help='Output path for vocabulary file'
    )

    parser.add_argument(
        '--train-ratio',
        type=float,
        default=0.70,
        help='Training set ratio (default: 0.70)'
    )

    parser.add_argument(
        '--val-ratio',
        type=float,
        default=0.15,
        help='Validation set ratio (default: 0.15)'
    )

    parser.add_argument(
        '--test-ratio',
        type=float,
        default=0.15,
        help='Test set ratio (default: 0.15)'
    )

    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='Random seed for reproducibility (default: 42)'
    )

    args = parser.parse_args()

    setup_logging()
    logger = logging.getLogger(__name__)

    logger.info("=" * 80)
    logger.info(f"{'Dataset Preparation':^80}")
    logger.info("=" * 80)

    # Check if sensor data exists
    sensor_data_path = Path(args.sensor_data)

    if not sensor_data_path.exists():
        logger.error(f"Sensor data not found: {sensor_data_path}")
        logger.error("Please generate sensor data first:")
        logger.error("  python scripts/generate_synthetic_data.py")
        return 1

    # Load sensor data
    logger.info(f"Loading data from {sensor_data_path}...")
    df = pd.read_csv(sensor_data_path)

    logger.info(f"Loaded {len(df)} samples")
    logger.info(f"Columns: {df.columns.tolist()}")

    # Build vocabulary
    logger.info("\nBuilding vocabulary from templates...")

    vocab_path = Path(args.vocab_output)
    vocab = build_vocabulary_from_templates(
        num_samples_per_disease=100,
        seed=args.seed,
        save_path=vocab_path
    )

    logger.info(f"Vocabulary size: {len(vocab)} words")
    logger.info(f"Vocabulary saved to: {vocab_path}")

    # Generate explanations
    template_engine = TemplateEngine(seed=args.seed)
    df = generate_explanations(df, template_engine)

    # Create preprocessor and split data
    logger.info("\nCreating train/val/test splits...")

    preprocessor = DataPreprocessor(
        data_dir=Path(args.output_dir),
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        random_seed=args.seed
    )

    train_df, val_df, test_df = preprocessor.stratified_split(df, label_column='disease')

    # Save splits
    output_dir = Path(args.output_dir)
    preprocessor.save_splits(train_df, val_df, test_df, output_dir)

    # Print summary
    logger.info("\n" + "=" * 80)
    logger.info("âœ“ Dataset preparation complete!")
    logger.info("=" * 80)

    logger.info(f"\nOutput directory: {output_dir}")
    logger.info(f"  - train.csv: {len(train_df)} samples")
    logger.info(f"  - val.csv: {len(val_df)} samples")
    logger.info(f"  - test.csv: {len(test_df)} samples")
    logger.info(f"  - split_metadata.json")

    logger.info(f"\nVocabulary: {vocab_path}")
    logger.info(f"  - Size: {len(vocab)} words")

    logger.info("\nDataset columns:")
    logger.info(f"  {train_df.columns.tolist()}")

    logger.info("\nClass distribution:")
    logger.info("\nTrain:")
    logger.info(f"{train_df['disease'].value_counts()}")
    logger.info("\nValidation:")
    logger.info(f"{val_df['disease'].value_counts()}")
    logger.info("\nTest:")
    logger.info(f"{test_df['disease'].value_counts()}")

    logger.info("\n" + "=" * 80)
    logger.info("Next steps:")
    logger.info("=" * 80)
    logger.info("1. Train the model: python scripts/train.py")
    logger.info("2. Evaluate the model: python scripts/evaluate.py")

    return 0


if __name__ == "__main__":
    sys.exit(main())
